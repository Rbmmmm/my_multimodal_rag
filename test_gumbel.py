# æ–‡ä»¶è·¯å¾„: my_multimodal_rag/test_real_query.py

import torch

try:
    from src.models.gumbel_selector import GumbelModalSelector
except ImportError:
    print("é”™è¯¯: æ— æ³•æ‰¾åˆ° 'GumbelModalSelector'ã€‚è¯·ç¡®ä¿ 'src/models/gumbel_selector.py' æ–‡ä»¶å­˜åœ¨ä¸”è·¯å¾„æ­£ç¡®ã€‚")
    exit()

try:
    from src.utils.embedding_utils import get_text_embedding, MODEL_NAME
except ImportError:
    print("é”™è¯¯: æ— æ³•æ‰¾åˆ° 'get_text_embedding'ã€‚è¯·ç¡®ä¿ 'src/utils/embedding_utils.py' æ–‡ä»¶å­˜åœ¨ä¸”è·¯å¾„æ­£ç¡®ã€‚")
    exit()


def run_real_query_test():
    """ä½¿ç”¨ä¸€ä¸ªçœŸå®çš„æŸ¥è¯¢å’ŒçœŸå®çš„åµŒå…¥æ¨¡å‹æ¥è¯¦ç»†æ¼”ç¤ºå·¥ä½œæµç¨‹ã€‚"""
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹ä½¿ç”¨çœŸå®Queryå’ŒçœŸå®åµŒå…¥æ¨¡å‹è¿›è¡ŒGumbelç½‘ç»œæµ‹è¯•")
    print("="*60)
    
    # Gumbelé€‰æ‹©å™¨çš„è¾“å…¥ç»´åº¦éœ€è¦å’ŒåµŒå…¥æ¨¡å‹çš„è¾“å‡ºç»´åº¦ä¸€è‡´
    embedding_dimension = 1024 # BGE-M3 çš„è¾“å‡ºç»´åº¦æ˜¯ 1024
    modal_options = ["æ–‡æœ¬æ£€ç´¢è·¯å¾„", "å›¾åƒæ£€ç´¢è·¯å¾„", "è¡¨æ ¼æ£€ç´¢è·¯å¾„"]
    
    # --- è¿™æ˜¯å·²ä¿®æ­£çš„éƒ¨åˆ† ---
    # 1. å®šä¹‰æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨çš„è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"å°†åœ¨æ­¤è®¾å¤‡ä¸Šè¿è¡ŒGumbelé€‰æ‹©å™¨: {device}")

    # 2. åˆ›å»ºæ¨¡å‹å®ä¾‹
    selector = GumbelModalSelector(input_dim=embedding_dimension, num_choices=len(modal_options))
    
    # 3. å°†æ•´ä¸ªæ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ï¼
    selector.to(device)

    # 4. å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    selector.eval()
    # --- ä¿®æ­£ç»“æŸ ---

    real_query = "Apply for Nordic Swan Ecolabel license, what is recommended as a web browser according to the Nordic Ecolabelling Portal instructions?"
    print(f"\n[å¾…å¤„ç†æŸ¥è¯¢]: '{real_query}'")
    print("-" * 60)

    # --- æ­¥éª¤ 1: å°†æ–‡æœ¬æŸ¥è¯¢è½¬æ¢ä¸ºçœŸå®çš„åµŒå…¥å‘é‡ ---
    print(f"[æ­¥éª¤ 1/4]: æ–‡æœ¬æŸ¥è¯¢ -> çœŸå®åµŒå…¥å‘é‡ (ä½¿ç”¨ {MODEL_NAME})")
    try:
        # get_text_embedding å‡½æ•°å†…éƒ¨å·²ç»å¤„ç†äº†è®¾å¤‡ï¼Œæ‰€ä»¥è¿”å›çš„å‘é‡åœ¨GPUä¸Š
        query_embedding = get_text_embedding(real_query)
        print(f"âœ… æˆåŠŸç”ŸæˆæŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶: {query_embedding.shape}")
    except Exception as e:
        print(f"âŒ ç”ŸæˆæŸ¥è¯¢å‘é‡å¤±è´¥: {e}")
        return
    print("-" * 60)

    # --- æ­¥éª¤ 2: è·å–åŸå§‹æ‰“åˆ† (Logits) ---
    print("[æ­¥éª¤ 2/4]: åµŒå…¥å‘é‡ -> åŸå§‹åˆ†æ•° (Logits)")
    with torch.no_grad():
        # ç°åœ¨æ¨¡å‹å’Œæ•°æ®éƒ½åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Šï¼Œè¿™è¡Œä»£ç å°†èƒ½æ­£å¸¸è¿è¡Œ
        logits = selector.classifier(query_embedding)
    print(f"âœ… æ¨¡å‹ç”Ÿæˆçš„åŸå§‹åˆ†æ•°: {logits.numpy(force=True).flatten()}") # ä½¿ç”¨ force=True ä»GPUå®‰å…¨åœ°ç§»åˆ°CPUè¿›è¡Œæ‰“å°
    print("-" * 60)
    
    # --- æ­¥éª¤ 3: Gumbel-Softmax å†³ç­– ---
    print("[æ­¥éª¤ 3/4]: Logits -> Gumbel-Softmax -> ç‹¬çƒ­å‘é‡å†³ç­–")
    with torch.no_grad():
        selection_one_hot = selector(query_embedding, temperature=1.0)
    print(f"âœ… ç”Ÿæˆçš„ç‹¬çƒ­å‘é‡å†³ç­–: {selection_one_hot.numpy(force=True).flatten()}")
    print("-" * 60)

    # --- æ­¥éª¤ 4: è§£ææœ€ç»ˆç»“æœ ---
    print("[æ­¥éª¤ 4/4]: ç‹¬çƒ­å‘é‡ -> äººç±»å¯è¯»çš„ç»“æœ")
    choice_index = selection_one_hot.argmax().item()
    final_decision = modal_options[choice_index]
    
    print("\n" + "="*25)
    print("ğŸ‰ æœ€ç»ˆå†³ç­–ç»“æœ ğŸ‰")
    print(f"å¯¹äºæŸ¥è¯¢: '{real_query}'")
    print(f"Gumbelç½‘ç»œå†³ç­–çš„æœ€ç»ˆç»“æœæ˜¯: ã€{final_decision}ã€‘")
    print("="*25)

if __name__ == '__main__':
    run_real_query_test()