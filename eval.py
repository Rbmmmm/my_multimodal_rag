import os
import json
import torch
import argparse
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# ===============================================================
# 1. å¯¼å…¥æ‚¨é¡¹ç›®çš„ç»„ä»¶å’ŒåŸå§‹ eval è„šæœ¬çš„ä¾èµ–
# ===============================================================
# å¯¼å…¥æ‚¨çš„é¡¹ç›®ç»„ä»¶
from src.searcher.text_searcher import TextSearcher
from src.searcher.image_searcher import ImageSearcher
from src.searcher.table_searcher import TableSearcher
from src.searcher.SearchEngine import SearchEngine
from src.agent.seeker_agent import SeekerAgent
from src.agent.inspector_agent import InspectorAgent
from src.agent.synthesizer_agent import SynthesizerAgent
from src.models.gumbel_selector import GumbelModalSelector
from src.orchestrator import RAGOrchestrator
from src.llms.llm import LLM
from src.utils.embedding_utils import QueryEmbedder, get_query_embedding

# å¯¼å…¥åŸå§‹ eval è„šæœ¬çš„ä¾èµ–
from src.llms.evaluator import Evaluator # å‡è®¾æ‚¨å·²å°† ViDoRAG çš„ evaluator.py æ”¾åœ¨ llms/ ç›®å½•ä¸‹
from src.utils.overall_evaluator import eval_search, eval_search_type_wise # å‡è®¾è¯„ä¼°å‡½æ•°åœ¨ utils/ ç›®å½•ä¸‹

from llama_index.core import Settings
Settings.llm = None


class MMRAG:
    """
    è¯„ä¼°æ¡†æ¶ä¸»ç±»ï¼Œç»“æ„å’Œé€»è¾‘ä¸æ‚¨æä¾›çš„ ViDoRAG eval.py ä¿æŒä¸€è‡´ã€‚
    ä»…ä¿®æ”¹äº†åˆå§‹åŒ–å’Œ RAG è°ƒç”¨éƒ¨åˆ†ä»¥é€‚é…æ‚¨çš„é¡¹ç›®ã€‚
    """
    def __init__(self, args):
        # --- åŸºç¡€é…ç½® ---
        self.args = args
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.dataset_dir = os.path.join('./data', args.dataset)
        self.results_dir = os.path.join(self.dataset_dir, "results_final") # ä½¿ç”¨æ–°ç›®å½•ä»¥åŒºåˆ†
        os.makedirs(self.results_dir, exist_ok=True)

        # ==========================================================================
        # æ­¥éª¤ 1: åˆå§‹åŒ–æ‰€æœ‰ç³»ç»Ÿç»„ä»¶ (é€‚é…æ‚¨çš„é¡¹ç›®)
        # ==========================================================================
        print("="*30)
        print("æ­¥éª¤ 1: åˆå§‹åŒ–æ‰€æœ‰ç³»ç»Ÿç»„ä»¶")

        # --- 1a. åˆå§‹åŒ–æ¨¡å‹ ---
        print("\n[1a] åˆå§‹åŒ–åµŒå…¥å’Œ VLM æ¨¡å‹...")
        self.query_embedder = QueryEmbedder(model_name=args.embed_model_name, device=self.device)
        self.vlm = LLM(args.generate_vlm)
        self.evaluator = Evaluator() # ä½¿ç”¨åŒä¸€ä¸ª VLM è¿›è¡Œè¯„ä¼°
        print("[1a] âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ.")

        # --- 1b. åˆå§‹åŒ– SearchEngine ---
        print("\n[1b] åˆå§‹åŒ– SearchEngine...")
        retriever_factories = {
            "text": lambda: TextSearcher(dataset_name=args.dataset, mode='bi_encoder'),
            "image": lambda: ImageSearcher(dataset_name=args.dataset, mode='vl_search'),
            "table": lambda: TableSearcher(dataset_name=args.dataset, mode='vl_search')
        }
        self.search_engine = SearchEngine(retriever_factories=retriever_factories)
        print("[1b] âœ… SearchEngine åˆå§‹åŒ–å®Œæˆ.")

        # --- 1c. åˆå§‹åŒ– Gumbel é€‰æ‹©å™¨ ---
        print("\n[1c] åˆå§‹åŒ– Gumbel æ¨¡æ€é€‰æ‹©å™¨...")
        gumbel_selector = GumbelModalSelector(
            input_dim=self.query_embedder.out_dim, num_choices=3, hidden_dim=256
        ).to(self.device)
        if os.path.exists(args.selector_ckpt):
            gumbel_selector.load_state_dict(torch.load(args.selector_ckpt, map_location=self.device))
            print(f"[1c] âœ… Gumbel Selector æƒé‡ä» {args.selector_ckpt} åŠ è½½æˆåŠŸ.")
        else:
            print(f"[1c] â„¹ï¸ æœªæ‰¾åˆ° Gumbel Selector æƒé‡ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–.")
        gumbel_selector.eval()

        # --- 1d. åˆå§‹åŒ– Agents å’Œ Orchestrator ---
        print("\n[1d] åˆå§‹åŒ– Agents å’Œ Orchestrator...")
        image_base_dir = f"data/{args.dataset}/img"
        seeker_agent = SeekerAgent(vlm=self.vlm, image_base_dir=image_base_dir)
        inspector_agent = InspectorAgent(vlm=self.vlm, image_base_dir=image_base_dir, reranker_model_name="BAAI/bge-reranker-large")
        synthesizer_agent = SynthesizerAgent(vlm=self.vlm, image_base_dir=image_base_dir)
        
        self.orchestrator = RAGOrchestrator(
            search_engine=self.search_engine, seeker=seeker_agent, inspector=inspector_agent,
            synthesizer=synthesizer_agent, gumbel_selector=gumbel_selector
        )
        print("[1d] âœ… Agents å’Œ Orchestrator åˆå§‹åŒ–å®Œæˆ.")
        
        # ==========================================================================
        # æ­¥éª¤ 2: æ ¹æ®å®éªŒç±»å‹è®¾ç½®è¯„ä¼°å‡½æ•°å’Œè¾“å‡ºæ–‡ä»¶å
        # ==========================================================================
        print("\næ­¥éª¤ 2: é…ç½®å®éªŒç±»å‹")
        if args.experiment_type == 'retrieval_infer':
            self.eval_func = self.retrieval_infer
            self.output_file_name = f'retrieval_{args.embed_model_name}.jsonl'
        elif args.experiment_type == 'vidorag':
            self.eval_func = self.vidorag_infer
            self.output_file_name = f'vidorag_{args.generate_vlm}.jsonl'
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å®éªŒç±»å‹: {args.experiment_type}")

        self.output_file_path = os.path.join(self.results_dir, self.output_file_name.replace("/", "-"))
        print(f"âœ… å®éªŒç±»å‹: '{args.experiment_type}' | è¾“å‡ºæ–‡ä»¶: {self.output_file_path}")
        print("="*30)

    def retrieval_infer(self, sample):
        query = sample['query']
        # æ³¨æ„ï¼šè¿™é‡Œçš„ search é€»è¾‘å¯èƒ½éœ€è¦æ ¹æ®æ‚¨çš„ SearchEngine å®ç°è¿›è¡Œå¾®è°ƒ
        # è¿™é‡Œå‡è®¾ search ç›´æ¥è¿”å› LlamaIndex çš„ NodeWithScore åˆ—è¡¨
        retrieved_nodes = self.search_engine.search(query, modality='text', top_k=self.args.topk) # é»˜è®¤ç”¨ text æ£€ç´¢
        
        # å°†æ£€ç´¢ç»“æœè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸æ ¼å¼
        sample['recall_results'] = {
            "source_nodes": [node.to_dict() for node in retrieved_nodes]
        }
        return sample

    def vidorag_infer(self, sample):
        query = sample['query']
        print(f"\nProcessing query: {query}")
        try:
            query_embedding = get_query_embedding(self.query_embedder, query)
            answer = self.orchestrator.run(
                query=query,
                query_embedding=query_embedding,
                initial_top_k=self.args.topk
            )
            # ç”±äº Orchestrator å†…éƒ¨å°è£…äº†æ£€ç´¢ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæ— æ³•ç›´æ¥è·å–å¬å›çš„èŠ‚ç‚¹
            # è¯„ä¼°å°†ä¸»è¦å…³æ³¨æœ€ç»ˆç­”æ¡ˆçš„è´¨é‡
            sample['recall_results'] = {"source_nodes": []} # ç•™ç©ºæˆ–å¡«å……ä¼ªæ•°æ®
        except Exception as e:
            print(f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
        
        # ä½¿ç”¨ Evaluator è¯„ä¼°ç­”æ¡ˆ
        sample['eval_result'] = self.evaluator.evaluate(query, sample['reference_answer'], str(answer))
        sample['response'] = answer
        return sample

    def eval_dataset(self):
        rag_dataset_path = os.path.join(self.dataset_dir, self.args.query_file)
        with open(rag_dataset_path, "r", encoding="utf-8") as f:
            data = json.load(f)['examples']
        
        # æ–­ç‚¹ç»­è¯„é€»è¾‘
        if os.path.exists(self.output_file_path):
            print("â„¹ï¸ å‘ç°å·²å­˜åœ¨çš„è¯„ä¼°ç»“æœæ–‡ä»¶ï¼Œå°†è¿›è¡Œæ–­ç‚¹ç»­è¯„...")
            with open(self.output_file_path, "r", encoding="utf-8") as f:
                completed_uids = {json.loads(line)["uid"] for line in f}
            data = [item for item in data if item['uid'] not in completed_uids]
            print(f"âœ… å·²å®Œæˆ {len(completed_uids)} ä¸ªæ ·æœ¬ï¼Œå‰©ä½™ {len(data)} ä¸ªå¾…è¯„ä¼°ã€‚")

        # å•çº¿ç¨‹æˆ–å¤šçº¿ç¨‹å¤„ç†
        if self.args.workers_num == 1:
            for item in tqdm(data, desc="Processing Samples"):
                result = self.eval_func(item)
                if result:
                    with open(self.output_file_path, "a", encoding="utf-8") as f:
                        f.write(json.dumps(result, ensure_ascii=False) + "\n")
        else:
            with ThreadPoolExecutor(max_workers=self.args.workers_num) as executor:
                futures = [executor.submit(self.eval_func, item) for item in data]
                for future in tqdm(as_completed(futures), total=len(futures), desc="Processing Samples"):
                    result = future.result()
                    if result:
                        with open(self.output_file_path, "a", encoding="utf-8") as f:
                            f.write(json.dumps(result, ensure_ascii=False) + "\n")
    
    def eval_overall(self):
        print("\nğŸ“Š è®¡ç®—æ•´ä½“è¯„ä¼°æŒ‡æ ‡...")
        data = [json.loads(line) for line in open(self.output_file_path, "r", encoding="utf-8")]
        results = eval_search(data) # ä¾èµ–æ‚¨çš„è¯„ä¼°å‡½æ•°
        with open(self.output_file_path.replace(".jsonl", "_eval.json"), "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"âœ… æ•´ä½“è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜ã€‚")
    
    def eval_overall_type_wise(self):
        print("\nğŸ“Š è®¡ç®—åˆ†ç±»åˆ«è¯„ä¼°æŒ‡æ ‡...")
        data = [json.loads(line) for line in open(self.output_file_path, "r", encoding="utf-8")]
        results = eval_search_type_wise(data) # ä¾èµ–æ‚¨çš„è¯„ä¼°å‡½æ•°
        with open(self.output_file_path.replace(".jsonl", "_eval_type_wise.json"), "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"âœ… åˆ†ç±»åˆ«è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜ã€‚")

def arg_parse():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str, default='ViDoSeek', help="The name of dataset")
    parser.add_argument("--query_file", type=str, default='rag_dataset.json', help="The name of anno_file")
    parser.add_argument("--experiment_type", type=str, default='vidorag', help="The type of experiment ('vidorag' or 'retrieval_infer')")
    parser.add_argument("--embed_model_name", type=str, default='BAAI/bge-m3', help="The name of embedding model")
    parser.add_argument("--workers_num", type=int, default=1, help="The number of workers")
    parser.add_argument("--topk", type=int, default=10, help="The number of topk")
    parser.add_argument("--generate_vlm", type=str, default='qwen-vl-max', help="The name of VLM model")
    parser.add_argument("--selector_ckpt", type=str, default="checkpoints/modal_selector_best.pt", help="Path to Gumbel Selector checkpoint.")
    return parser.parse_args()

if __name__ == "__main__":
    args = arg_parse()
    mmrag = MMRAG(args)
    mmrag.eval_dataset()
    mmrag.eval_overall()
    mmrag.eval_overall_type_wise()