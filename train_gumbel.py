import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
from typing import List, Tuple
from tqdm import tqdm
import numpy as np
from sklearn.model_selection import train_test_split

# å‡è®¾ GumbelModalSelector çš„å®šä¹‰åœ¨è¿™é‡Œ
# ä¸ºäº†å®Œæ•´æ€§ï¼Œæˆ‘æ ¹æ®è®ºæ–‡è¡¥å……ä¸€ä¸ªå¯èƒ½çš„å®ç°
class GumbelModalSelector(nn.Module):
    def __init__(self, input_dim: int, num_choices: int, hidden_dim: int = 256, tau: float = 1.0, trainable: bool = True):
        super().__init__()
        self.tau = tau
        self.trainable = trainable
        # <<< æ”¹è¿›ç‚¹ 3: å¢åŠ æ¨¡å‹å¤æ‚åº¦ï¼Œä½¿ç”¨ MLP è€Œéå•å±‚çº¿æ€§
        self.selector_net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, num_choices)
        )
        if not trainable:
            for param in self.parameters():
                param.requires_grad = False

    def forward(self, x: torch.Tensor, hard: bool = True):
        logits = self.selector_net(x)
        if not self.training and hard: # æ¨ç†æ—¶ç›´æ¥ç”¨ argmax
            y_onehot = F.one_hot(torch.argmax(logits, dim=-1), num_classes=logits.shape[-1]).float()
            return y_onehot, y_onehot, logits

        # è®­ç»ƒæ—¶ä½¿ç”¨ Gumbel-Softmax
        y_onehot = F.gumbel_softmax(logits, tau=self.tau, hard=hard, dim=-1)
        probs = F.softmax(logits, dim=-1) # ç”¨äºè®¡ç®— acc çš„æ¦‚ç‡
        return probs, y_onehot, logits

from src.utils.embedding_utils import QueryEmbedder
# from src.models.gumbel_selector import GumbelModalSelector # ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„ç‰ˆæœ¬

# ========= Config =========
DATA_PATH = "data/ViDoSeek/rag_dataset.json"
SAVE_PATH = "checkpoints/modal_selector_best.pt"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# <<< æ”¹è¿›ç‚¹ 3: è°ƒæ•´è¶…å‚æ•°
BATCH_SIZE = 16 # å¯ä»¥é€‚å½“å¢å¤§
EPOCHS = 20 # å¢åŠ  Epoch æ•°é‡ï¼Œé…åˆ Early Stopping
LR = 3e-5 # AdamW æ›´é€‚åˆé…åˆæƒé‡è¡°å‡
WEIGHT_DECAY = 0.01
TAU_START = 2.0 # Gumbel æ¸©åº¦åˆå§‹å€¼
TAU_END = 0.5 # Gumbel æ¸©åº¦æœ€ç»ˆå€¼

CLASS_NAMES = ["text", "2d_layout", "chart"]
LABEL_ALIAS = {
    "text": "text", "2d_layout": "2d_layout",
    "chart": "chart", "table": "chart",
}

def to_class_id(source_type: str) -> int | None:
    if not isinstance(source_type, str): return None
    st = LABEL_ALIAS.get(source_type.strip().lower())
    if st is None: return None
    return CLASS_NAMES.index(st)

# ========= Dataset =========
# <<< æ”¹è¿›ç‚¹ 4: ä¼˜åŒ– Datasetï¼Œæ”¯æŒé¢„è®¡ç®—çš„åµŒå…¥
class SelectorDataset(Dataset):
    def __init__(self, data_path: str, embedder: QueryEmbedder, device: str):
        with open(data_path, "r", encoding="utf-8") as f:
            raw = json.load(f)
        data = raw.get("examples", raw)

        self.embeddings: List[torch.Tensor] = []
        self.labels: List[int] = []
        counts = {k: 0 for k in CLASS_NAMES}
        skipped = 0

        print("Pre-computing embeddings...")
        for ex in tqdm(data):
            q = ex.get("query", "")
            cid = to_class_id(ex.get("meta_info", {}).get("source_type"))
            if cid is not None:
                with torch.no_grad():
                    # è®¡ç®—åµŒå…¥å¹¶ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
                    emb = embedder.encode(q)
                    if not isinstance(emb, torch.Tensor):
                        emb = torch.tensor(emb)
                    self.embeddings.append(emb.float().to('cpu')) # å­˜å‚¨åœ¨CPUå†…å­˜ä¸­
                self.labels.append(cid)
                counts[CLASS_NAMES[cid]] += 1
            else:
                skipped += 1
        
        self.labels = np.array(self.labels)
        print(f"Loaded {len(self.labels)} samples | class counts={counts} | skipped={skipped}")

    def __len__(self) -> int:
        return len(self.labels)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        return self.embeddings[idx], self.labels[idx]

# ========= Train & Eval Loop =========
def train_one_epoch(model, dataloader, optimizer, scheduler, criterion, device, current_tau):
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0

    for embs, labels in tqdm(dataloader, desc="Training"):
        embs, labels = embs.to(device), labels.to(device)
        
        model.tau = current_tau # <<< æ”¹è¿›ç‚¹ 3: æ›´æ–° Gumbel æ¸©åº¦
        probs, _, logits = model(embs, hard=True)
        loss = criterion(logits, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step() # <<< æ”¹è¿›ç‚¹ 3: æ›´æ–°å­¦ä¹ ç‡

        total_loss += loss.item() * labels.size(0)
        pred = probs.argmax(dim=-1)
        correct += (pred == labels).sum().item()
        total += labels.size(0)

    avg_loss = total_loss / total
    acc = 100.0 * correct / total
    return avg_loss, acc

def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for embs, labels in tqdm(dataloader, desc="Evaluating"):
            embs, labels = embs.to(device), labels.to(device)
            
            probs, _, logits = model(embs, hard=True)
            loss = criterion(logits, labels)

            total_loss += loss.item() * labels.size(0)
            pred = probs.argmax(dim=-1)
            correct += (pred == labels).sum().item()
            total += labels.size(0)
            
    avg_loss = total_loss / total
    acc = 100.0 * correct / total
    return avg_loss, acc


def train():
    print("ğŸš€ Training Gumbel Selector with supervised labels")

    # 1) åµŒå…¥å™¨
    embedder = QueryEmbedder(model_name="BAAI/bge-m3", device=DEVICE)

    # 2) æ•°æ®é›†å’Œé¢„è®¡ç®—
    full_dataset = SelectorDataset(DATA_PATH, embedder, DEVICE)

    # <<< æ”¹è¿›ç‚¹ 2: åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_indices, val_indices = train_test_split(
        range(len(full_dataset)),
        test_size=0.2,
        random_state=42,
        stratify=full_dataset.labels # ç¡®ä¿åˆ†å±‚æŠ½æ ·
    )
    train_dataset = Subset(full_dataset, train_indices)
    val_dataset = Subset(full_dataset, val_indices)

    # collate_fn ç°åœ¨å¾ˆç®€å•ï¼Œå› ä¸ºæ•°æ®å·²ç»æ˜¯ Tensor
    def collate_fn(batch):
        embs = torch.stack([item[0] for item in batch])
        labels = torch.tensor([item[1] for item in batch], dtype=torch.long)
        return embs, labels

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)
    print(f"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}")

    # 3) æ¨¡å‹
    selector = GumbelModalSelector(
        input_dim=embedder.out_dim,
        num_choices=len(CLASS_NAMES),
        tau=TAU_START,
    ).to(DEVICE)
    
    # <<< æ”¹è¿›ç‚¹ 1: è®¡ç®—ç±»åˆ«æƒé‡
    class_counts = np.bincount(full_dataset.labels[train_indices])
    class_weights = 1. / torch.tensor(class_counts, dtype=torch.float32)
    class_weights = class_weights / class_weights.sum()
    class_weights = class_weights.to(DEVICE)
    print(f"Using class weights: {class_weights.cpu().numpy()}")
    
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = torch.optim.AdamW(selector.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    
    # <<< æ”¹è¿›ç‚¹ 3: è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader) * EPOCHS)

    best_val_acc = 0.0
    for epoch in range(1, EPOCHS + 1):
        # <<< æ”¹è¿›ç‚¹ 3: æ¸©åº¦é€€ç«
        current_tau = TAU_START - (TAU_START - TAU_END) * (epoch / EPOCHS)
        
        print(f"\nğŸ“š Epoch {epoch}/{EPOCHS} | LR: {optimizer.param_groups[0]['lr']:.6f} | Tau: {current_tau:.4f}")
        
        train_loss, train_acc = train_one_epoch(selector, train_loader, optimizer, scheduler, criterion, DEVICE, current_tau)
        print(f"âœ… Train | Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%")
        
        val_loss, val_acc = evaluate(selector, val_loader, criterion, DEVICE)
        print(f"ğŸ“Š Val   | Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%")
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)
            torch.save(selector.state_dict(), SAVE_PATH)
            print(f"ğŸ’¾ New best model saved with Val Acc: {best_val_acc:.2f}% to {SAVE_PATH}")

    print(f"ğŸ‰ Training finished. Best Val Acc: {best_val_acc:.2f}%")

if __name__ == "__main__":
    train()