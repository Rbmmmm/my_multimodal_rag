import os
import json
import torch

from src.retrievers.text_retriever import TextRetriever
from src.retrievers.image_retriever import ImageRetriever
from src.retrievers.chart_retriever import ChartRetriever

from src.agents.seeker_agent import SeekerAgent
from src.agents.inspector_agent import InspectorAgent
from src.agents.synthesizer_agent import SynthesizerAgent

from src.models.gumbel_selector import GumbelModalSelector
from src.orchestrator import RAGOrchestrator
from src.utils.embedding_utils import QueryEmbedder, get_query_embedding

# ====== å¯é€‰ï¼šQwen-VL å®¢æˆ·ç«¯ ======
_qwen_vlm = None
try:
    # ä½ éœ€è¦æä¾› src/vlm/qwen_vlm.pyï¼ŒåŒ…å« QwenVLM ç±»ï¼ˆè§ä¹‹å‰è¯´æ˜ï¼‰
    from src.vlm.qwen_vlm import QwenVLM  # noqa
    if os.getenv("DASHSCOPE_API_KEY"):
        _qwen_vlm = QwenVLM(model="qwen-vl-max")  # ä¼šè‡ªåŠ¨è¯»å– DASHSCOPE_API_KEY
        print("âœ… Qwen-VL client initialized.")
    else:
        print("â„¹ï¸ DASHSCOPE_API_KEY not set. VLM path disabled (will fallback to text-only).")
except Exception as e:
    print(f"â„¹ï¸ Qwen-VL client not available: {e} (will fallback to text-only).")


def main():
    print("ğŸš€ Initializing Multi-Agent RAG System")

    # 1) è®¾å¤‡ä¸æŸ¥è¯¢åµŒå…¥æ¨¡å‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\n[1] åˆå§‹åŒ–æŸ¥è¯¢åµŒå…¥å™¨... (device={device})")
    q_embedder = QueryEmbedder(model_name="BAAI/bge-m3", device=device)

    # 2) åŠ è½½æ ·ä¾‹
    print("\n[2] åŠ è½½æµ‹è¯•æ ·æœ¬...")
    dataset_path = "data/ViDoSeek/rag_dataset.json"
    with open(dataset_path, "r", encoding="utf-8") as f:
        examples = json.load(f)["examples"]

    # æŒ‰ä½ è‡ªå·±çš„æµ‹è¯•èŠ‚å¥æ¥ï¼›è¿™é‡Œè·‘å‰ 3 æ¡
    test_num = 3

    for i in range(test_num):
        sample_idx = i
        sample = examples[sample_idx]
        query = sample["query"]

        # ç»Ÿä¸€çš„ Query Embedding
        query_embedding = get_query_embedding(q_embedder, query)

        # å¼ºåˆ¶æ¨¡æ€ï¼ˆ0=text, 1=image, 2=chartï¼‰ï¼Œä¸ºäº†å¯å¤ç°å®éªŒ
        src_type = sample.get("meta_info", {}).get("source_type")
        if src_type == "text":
            force_modality = 0
        elif src_type == "chart":
            force_modality = 1  # å›¾è¡¨ç›®å‰ä»èµ° image-OCR çº¿è·¯
        else:
            force_modality = 1  # image

        # 3) æ£€ç´¢å™¨æ‡’åŠ è½½å·¥å‚ï¼ˆä¸æå‰å®ä¾‹åŒ–ï¼Œé¿å…é‡å¤åˆå§‹åŒ–ä¸é¢„ç¼–ç ï¼‰
        print("\n[3] åˆå§‹åŒ–æ£€ç´¢å™¨ï¼ˆæŒ‰éœ€ï¼‰...")
        factories = {
            "text": lambda: TextRetriever(node_dir="data/ViDoSeek/bge_ingestion"),
            "image": lambda: ImageRetriever(
                node_dir="data/ViDoSeek/colqwen_ingestion",
                ocr_dir="data/ViDoSeek/bge_ingestion",
                text_encoder="BAAI/bge-m3",
                cache_dir=".cache/ocr_embeds",   # âœ… ä¿ç•™ç¼“å­˜ç›®å½•
            ),
            "chart": lambda: ChartRetriever(node_dir="data/ViDoSeek/bge_ingestion"),
        }

        # ä¸æå‰å»ºï¼Œäº¤ç»™ orchestrator éœ€è¦æ—¶å†å»º
        text_retriever = None
        image_retriever = None
        chart_retriever = None

        # 4) ç»„è£…æ™ºèƒ½ä½“ä¸æ¨¡æ€é€‰æ‹©å™¨
        print("\n[4] ç»„è£…æ™ºèƒ½ä½“ä¸æ¨¡æ€é€‰æ‹©å™¨...")
        selector = GumbelModalSelector(
            input_dim=q_embedder.out_dim,  # 1024
            hidden_dim=0,
            num_choices=2,  # ç›®å‰åª text / image
            tau=1.0,
        )

        ckpt_path = "checkpoints/modal_selector_text_image.pt"
        if os.path.exists(ckpt_path):
            state = torch.load(ckpt_path, map_location="cpu")
            selector.load_state_dict(state, strict=False)
            selector.eval()
            print(f"âœ… Loaded selector weights from {ckpt_path}")
        else:
            print("â„¹ï¸ No selector checkpoint found, using randomly initialized selector.")

        seeker = SeekerAgent(
            text_retriever=text_retriever,
            image_retriever=image_retriever,
            chart_retriever=chart_retriever,
        )
        inspector = InspectorAgent(
            reranker_model_name="BAAI/bge-reranker-large",
            heuristic_enable=True,
            default_conf_threshold=0.15,
        )
        # å°† VLM å®¢æˆ·ç«¯æ³¨å…¥ Synthesizerï¼ˆä»…åœ¨æœ‰å›¾ç‰‡è¯æ®æ—¶ä¼šè¢«è°ƒç”¨ï¼‰
        synthesizer = SynthesizerAgent(
            model_name="google/flan-t5-large",
            vlm_client=_qwen_vlm,
            use_vlm=True,
        )

        orchestrator = RAGOrchestrator(
            seeker=seeker,
            inspector=inspector,
            synthesizer=synthesizer,
            gumbel_selector=selector,
            use_modal_selector=True,
            # å…³é”®ï¼šæ°¸è¿œæŠŠä¸‰ç§ factory éƒ½ä¼ ä¸‹å»ï¼ŒçœŸæ­£ç”¨åˆ°å“ªä¸ªå†æ‡’åŠ è½½
            lazy_init_factories=factories,
        )

        print("\n" + "=" * 60)
        print(f"ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼ŒæŸ¥è¯¢: '{query}'")
        print("=" * 60)

        # 5) è¿è¡Œç¼–æ’å™¨
        final_answer = orchestrator.run(query, query_embedding, force_modality=force_modality)

        # 6) æ‰“å°ç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ‰ RAG æµç¨‹æ‰§è¡Œå®Œæ¯• ğŸ‰")
        print(f"\n[ç”¨æˆ·é—®é¢˜]: {query}")
        print("-" * 30)
        print(f"[æ¨¡å‹ç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆ]:\n{final_answer}")
        print("-" * 30)
        print(f"[æ•°æ®ä¸­çš„å‚è€ƒç­”æ¡ˆ]:\n{sample.get('reference_answer', '(no reference)')}")
        print("=" * 60)


if __name__ == "__main__":
    main()