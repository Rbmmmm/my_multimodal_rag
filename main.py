# File: my_multimodal_rag/main.py

import json
import os
import torch

from src.retrievers.text_retriever import TextRetriever
from src.retrievers.image_retriever import ImageRetriever
from src.retrievers.chart_retriever import ChartRetriever

from src.agents.seeker_agent import SeekerAgent
from src.agents.inspector_agent import InspectorAgent
from src.agents.synthesizer_agent import SynthesizerAgent

from src.models.gumbel_selector import GumbelModalSelector
from src.orchestrator import RAGOrchestrator
from src.utils.embedding_utils import QueryEmbedder, get_query_embedding


def main():
    print("ğŸš€ Initializing Multi-Agent RAG System")

    # -------------------------------
    # 1) è®¾å¤‡ä¸æŸ¥è¯¢åµŒå…¥æ¨¡å‹
    # -------------------------------
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\n[1] åˆå§‹åŒ–æŸ¥è¯¢åµŒå…¥å™¨... (device={device})")
    q_embedder = QueryEmbedder(model_name="BAAI/bge-m3", device=device)

    # -------------------------------
    # 2) åŠ è½½æ ·ä¾‹
    # -------------------------------
    print("\n[2] åŠ è½½æµ‹è¯•æ ·æœ¬...")
    dataset_path = "data/ViDoSeek/rag_dataset.json"
    with open(dataset_path, "r", encoding="utf-8") as f:
        examples = json.load(f)["examples"]

    sample_idx = 3
    sample = examples[sample_idx]
    query = sample["query"]

    # ç»Ÿä¸€çš„ Query Embedding
    query_embedding = get_query_embedding(q_embedder, query)

    # å¯é€‰ï¼šå¼ºåˆ¶æŸä¸€è·¯ï¼ˆ0=text, 1=image, 2=chartï¼‰ï¼›ä¸å¼ºåˆ¶ç”¨ None
    force_modality = 1  # â† ç°åœ¨å…ˆå‹æµ‹ image

    # -------------------------------
    # 3) æ£€ç´¢å™¨æŒ‰éœ€åˆå§‹åŒ–ï¼ˆåªç«‹å³å»ºè¢«å¼ºåˆ¶é‚£ä¸€è·¯ï¼Œå…¶ä½™äº¤ç»™æ‡’åŠ è½½ï¼‰
    # -------------------------------
    print("\n[3] åˆå§‹åŒ–æ£€ç´¢å™¨ï¼ˆæŒ‰éœ€ï¼‰...")

    # ç»Ÿä¸€ç”¨å·¥å‚ï¼Œé˜²æ­¢é‡å¤å®ä¾‹åŒ–
    factories = {
        "text":  lambda: TextRetriever(node_dir="data/ViDoSeek/bge_ingestion"),
        "image": lambda: ImageRetriever(
            node_dir="data/ViDoSeek/colqwen_ingestion",
            ocr_dir="data/ViDoSeek/bge_ingestion",
            text_encoder="BAAI/bge-m3",
        ),
        "chart": lambda: ChartRetriever(node_dir="data/ViDoSeek/bge_ingestion"),
    }

    # ä»…å½“è¢«å¼ºåˆ¶æ—¶æ‰ç«‹å³å®ä¾‹åŒ–ï¼›å¦åˆ™ç•™ç»™ orchestrator æ‡’åŠ è½½
    text_retriever  = factories["text"]()  if force_modality == 0 else None
    image_retriever = factories["image"]() if force_modality == 1 else None
    chart_retriever = factories["chart"]() if force_modality == 2 else None

    # -------------------------------
    # 4) ç»„è£…æ™ºèƒ½ä½“ä¸æ¨¡æ€é€‰æ‹©å™¨
    # -------------------------------
    print("\n[4] ç»„è£…æ™ºèƒ½ä½“ä¸æ¨¡æ€é€‰æ‹©å™¨...")
    selector = GumbelModalSelector(
        input_dim=q_embedder.out_dim,  # 1024
        hidden_dim=0,
        num_choices=2,                 # ç°åœ¨åª text / image
        tau=1.0
    )

    ckpt_path = "checkpoints/modal_selector_text_image.pt"
    if os.path.exists(ckpt_path):
        state = torch.load(ckpt_path, map_localtion="cpu")  # å…¼å®¹ä¸åŒç¯å¢ƒ
        selector.load_state_dict(state, strict=False)
        selector.eval()
        print(f"âœ… Loaded selector weights from {ckpt_path}")
    else:
        print("â„¹ï¸ No selector checkpoint found, using randomly initialized selector.")

    seeker = SeekerAgent(
        text_retriever=text_retriever,
        image_retriever=image_retriever,
        chart_retriever=chart_retriever,
    )
    inspector = InspectorAgent(  # å†…éƒ¨å·²å«æ»‘çª— + å¯å‘å¼
        # é»˜è®¤ä½¿ç”¨ BAAI/bge-reranker-large
        # å¯æŒ‰éœ€è¦†ç›– window_tokens / stride / batch_size
    )
    synthesizer = SynthesizerAgent(model_name="google/flan-t5-large")

    orchestrator = RAGOrchestrator(
        seeker=seeker,
        inspector=inspector,
        synthesizer=synthesizer,
        gumbel_selector=selector,
        use_modal_selector=True,
        # å…³é”®ï¼šæŠŠå·¥å‚ä¼ ä¸‹å»ï¼ŒçœŸæ­£ç”¨åˆ°å†åˆå§‹åŒ–ï¼ˆåªå»ºä¸€æ¬¡ï¼‰
        lazy_init_factories=factories,
    )

    print("\n" + "=" * 60)
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼ŒæŸ¥è¯¢: '{query}'")
    print("=" * 60)

    # -------------------------------
    # 5) è¿è¡Œç¼–æ’å™¨
    # -------------------------------
    final_answer = orchestrator.run(query, query_embedding, force_modality=force_modality)

    # -------------------------------
    # 6) æ‰“å°ç»“æœ
    # -------------------------------
    print("\n" + "=" * 60)
    print("ğŸ‰ RAG æµç¨‹æ‰§è¡Œå®Œæ¯• ğŸ‰")
    print(f"\n[ç”¨æˆ·é—®é¢˜]: {query}")
    print("-" * 30)
    print(f"[æ¨¡å‹ç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆ]:\n{final_answer}")
    print("-" * 30)
    print(f"[æ•°æ®ä¸­çš„å‚è€ƒç­”æ¡ˆ]:\n{sample.get('reference_answer', '(no reference)')}")
    print("=" * 60)


if __name__ == "__main__":
    main()