# æ–‡ä»¶è·¯å¾„: my_multimodal_rag/main.py (TypeError ä¿®æ­£ç‰ˆ)

import json
import os
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from src.retrievers.text_retriever import TextRetriever
from src.retrievers.image_retriever import ImageRetriever
from src.retrievers.table_retriever import TableRetriever
from src.agents.seeker_agent import SeekerAgent
from src.agents.inspector_agent import InspectorAgent
from src.agents.synthesizer_agent import SynthesizerAgent
from src.models.gumbel_selector import GumbelModalSelector
from src.orchestrator import RAGOrchestrator
from src.utils.embedding_utils import get_text_embedding # ç¡®ä¿è¿™ä¸ªæ–‡ä»¶å­˜åœ¨

def main():
    print("ğŸš€ åˆå§‹åŒ–RAGå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ...")
    
    # --- 1. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ ---
    print("\n[1] åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
    # è¿™ä¸ªLlamaIndexåŒ…è£…å™¨ä¸»è¦ç»™TextRetrieverçš„ç´¢å¼•å™¨ä½¿ç”¨
    embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-m3", device="cuda")
    
    print("\n[2] åˆå§‹åŒ–å„è·¯å¾„æ£€ç´¢å™¨...")
    text_retriever = TextRetriever(node_dir="data/ViDoSeek/bge_ingestion")
    image_retriever = ImageRetriever() # å ä½ç¬¦
    table_retriever = TableRetriever() # å ä½ç¬¦
    
    # --- 2. ç»„è£…æ™ºèƒ½ä½“ ---
    print("\n[3] ç»„è£…æ™ºèƒ½ä½“...")
    gumbel_selector = GumbelModalSelector(input_dim=1024, num_choices=3) # bge-m3 çš„ç»´åº¦æ˜¯ 1024
    seeker = SeekerAgent(text_retriever, image_retriever, table_retriever)
    inspector = InspectorAgent()
    synthesizer = SynthesizerAgent(model_name="Qwen/Qwen2-1.5B-Instruct") # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹è¿›è¡Œè°ƒè¯•
    
    # --- 3. å®ä¾‹åŒ–æ€»æŒ‡æŒ¥ ---
    orchestrator = RAGOrchestrator(gumbel_selector, seeker, inspector, synthesizer)
    
    # --- 4. åŠ è½½æµ‹è¯•æ ·æœ¬å¹¶è¿è¡Œ ---
    print("\n[4] åŠ è½½æµ‹è¯•æ ·æœ¬...")
    dataset_path = "data/ViDoSeek/rag_dataset.json"
    with open(dataset_path, 'r', encoding='utf-8') as f:
        sample = json.load(f)['examples'][1]
    
    query = sample['query']
    
    print("\n" + "="*60)
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼ŒæŸ¥è¯¢: '{query}'")
    print("="*60)

    # ã€å…³é”®ä¿®æ­£ã€‘è°ƒç”¨get_text_embeddingæ—¶ä¸åº”ä¼ é€’model_nameå‚æ•°
    query_embedding = get_text_embedding(query)

    final_answer = orchestrator.run(query, query_embedding)

    # ... (å±•ç¤ºæœ€ç»ˆç»“æœ)
    print("\n" + "="*60)
    print("ğŸ‰ RAG æµç¨‹æ‰§è¡Œå®Œæ¯• ğŸ‰")
    print(f"\n[ç”¨æˆ·é—®é¢˜]: {query}")
    print("-" * 30)
    print(f"[æ¨¡å‹ç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆ]:\n{final_answer}")
    print("-" * 30)
    print(f"[æ•°æ®ä¸­çš„å‚è€ƒç­”æ¡ˆ]:\n{sample['reference_answer']}")
    print("="*60)

if __name__ == '__main__':
    main()