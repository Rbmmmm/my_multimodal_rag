# 文件路径: my_multimodal_rag/src/agents/synthesizer_agent.py (最终版)

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import List
from llama_index.core.schema import BaseNode

class SynthesizerAgent:
    def __init__(self, model_name: str):
        """
        Initialize the answer generation agent.
        :param model_name: Hugging Face model name for generation.
        """
        print(f"Synthesizer: Loading generation model: {model_name}...")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        print(f"✅ Generation model loaded successfully.")

    def generate(self, query: str, relevant_nodes: List[BaseNode]) -> str:
        """
        Generate the final answer based on the query and high-quality retrieved context.
        :param query: The original user query.
        :param relevant_nodes: List of high-confidence nodes selected by Inspector.
        :return: Final answer string generated by the model.
        """
        if not relevant_nodes:
            return "Insufficient information to generate an answer."

        context_str = "\n\n".join([node.get_content() for node in relevant_nodes])
        
        messages = [
            {"role": "system", "content": "You are a helpful assistant. Please provide a clear and concise answer to the user's question based only on the provided context. Do not use any prior knowledge."},
            {"role": "user", "content": f"Based on the following context:\n\n---\n{context_str}\n---\n\nPlease answer the question: {query}"}
        ]
        
        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        print("\n[Synthesizer] Generating final answer based on high-quality evidence...")

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        outputs = self.model.generate(**inputs, max_new_tokens=256, eos_token_id=self.tokenizer.eos_token_id)
        
        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer_only = full_text.split("assistant\n")[-1]
        
        return answer_only.strip()