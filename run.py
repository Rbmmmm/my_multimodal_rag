# File: run.py
# The final, clean entry point to initialize and run the entire RAG system.

import os
import torch

# --- å¯¼å…¥æ‚¨é¡¹ç›®ä¸­çš„æ‰€æœ‰æ ¸å¿ƒæ¨¡å— ---
from src.searcher.text_searcher import TextSearcher
from src.searcher.image_searcher import ImageSearcher
from src.searcher.table_searcher import TableSearcher
from src.searcher.SearchEngine import SearchEngine
from src.agent.seeker_agent import SeekerAgent
from src.agent.inspector_agent import InspectorAgent
from src.agent.synthesizer_agent import SynthesizerAgent
from src.models.gumbel_selector import GumbelModalSelector
from src.orchestrator import RAGOrchestrator
from src.llms.llm import LLM               # å‡è®¾æ‚¨æœ‰ä¸€ä¸ªVLMå®¢æˆ·ç«¯çš„å°è£…
from src.utils.embedding_utils import QueryEmbedder      # å‡è®¾æ‚¨æœ‰ä¸€ä¸ªæŸ¥è¯¢åµŒå…¥å™¨çš„å°è£…

from llama_index.core import Settings
import json

Settings.llm = None

def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºè®¾ç½®å’Œè¿è¡Œ RAG æµç¨‹ã€‚
    """
    # ==========================================================================
    # æ­¥éª¤ 1: åˆå§‹åŒ–æ‰€æœ‰ç³»ç»Ÿç»„ä»¶
    # ==========================================================================
    print("="*30)
    print("Step 1: Initializing System Components...")
    
    # --- 1a. è®¾ç½® API Key å’Œè®¾å¤‡ ---
    # !! é‡è¦ !!: è¯·å°†æ‚¨çš„ API Key å¡«å…¥æ­¤å¤„
    os.environ['DASHSCOPE_API_KEY'] = "sk-fdb11107afd1435398e9d40958af5e42"
    device = "cuda" if torch.cuda.is_available() else "cpu"

    if 'xxxx' in os.environ['DASHSCOPE_API_KEY']:
        print("âš ï¸ WARNING: DASHSCOPE_API_KEY is not set. VLM calls will fail.")

    # --- 1b. åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å‹ ---
    print("\nInitializing core models...")
    query_embedder = QueryEmbedder(model_name="BAAI/bge-m3", device=device)
    vlm = LLM("qwen-vl-max")

    # --- 1c. åˆ›å»ºæ£€ç´¢å™¨â€œå·¥å‚â€ (Retriever Factories) ---
    print("\n[1c] Defining retriever factories for lazy loading...")
    retriever_factories = {
        "text": lambda: TextSearcher(
            dataset_name='ViDoSeek',
            mode='bi_encoder',
            # --- ä¿®æ­£ï¼šä¸º ColBERT æ¨¡å¼æ˜ç¡®æä¾›åŒ…å« .node æ–‡ä»¶çš„ç›®å½•å‰ç¼€ ---
            # ColBERT éœ€è¦è¯»å–è¿™äº›åŸå§‹æ–‡æœ¬èŠ‚ç‚¹æ¥å»ºç«‹è‡ªå·±çš„ç´¢å¼•
            node_dir_prefix='bge_ingestion' 
        ),
        "image": lambda: ImageSearcher(
            dataset_name='ViDoSeek',
            mode='vl_search',
            # å›¾åƒæ£€ç´¢å™¨ä¹Ÿå¯èƒ½éœ€è¦æŒ‡å®šå…¶èŠ‚ç‚¹ç›®å½•
            vl_node_dir_prefix='colqwen_ingestion' 
        ),
        "table": lambda: TableSearcher(
            dataset_name='ViDoSeek',
            mode='vl_search',
            # è¡¨æ ¼æ£€ç´¢å™¨åŒç†
            vl_node_dir_prefix='colqwen_ingestion' # å‡è®¾è¡¨æ ¼å’Œå›¾åƒç”¨åŒä¸€å¥—å¤šæ¨¡æ€èŠ‚ç‚¹
        )
    }

    # --- 1d. åˆå§‹åŒ–æ”¯æŒæ‡’åŠ è½½çš„ SearchEngine ---
    # æ³¨æ„ï¼šè¿™é‡Œåªä¼ å…¥äº†å·¥å‚ï¼Œæ²¡æœ‰è¿›è¡Œä»»ä½•å®é™…çš„æ¨¡å‹åŠ è½½ï¼
    search_engine = SearchEngine(retriever_factories=retriever_factories)
    print("âœ… LAZY SearchEngine initialized.")

    # --- 1e. åˆå§‹åŒ– Gumbel æ¨¡æ€é€‰æ‹©å™¨ ---
    gumbel_selector = GumbelModalSelector(
        input_dim=query_embedder.out_dim,
        num_choices=3, # 0=text, 1=image, 2=table
        trainable=False
    ).to(device).eval()

    # (å¯é€‰) åŠ è½½è®­ç»ƒå¥½çš„é€‰æ‹©å™¨æƒé‡
    ckpt_path = "checkpoints/modal_selector.pt"
    if os.path.exists(ckpt_path):
        gumbel_selector.load_state_dict(torch.load(ckpt_path, map_location=device))
        print(f"âœ… Loaded Gumbel Selector weights from {ckpt_path}")
    else:
        print("â„¹ï¸ No Gumbel Selector checkpoint found, using random initialization.")

    print("\nInitializing Agents...")
    image_base_dir = "data/ViDoSeek/img" # å®šä¹‰å›¾ç‰‡æ‰€åœ¨çš„æ ¹ç›®å½•
    seeker_agent = SeekerAgent(vlm=vlm, image_base_dir=image_base_dir)
    inspector_agent = InspectorAgent(vlm=vlm, image_base_dir=image_base_dir, reranker_model_name="BAAI/bge-reranker-large")
    synthesizer_agent = SynthesizerAgent(vlm=vlm, image_base_dir=image_base_dir)
    print("âœ… All Agents initialized.")

    # --- 1f. ç»„è£…æ€»æŒ‡æŒ¥ (Orchestrator) ---
    orchestrator = RAGOrchestrator(
        search_engine=search_engine,
        seeker=seeker_agent,          # <-- ä½¿ç”¨å·²ç»åˆ›å»ºå¥½çš„å®ä¾‹
        inspector=inspector_agent,    # <-- ä½¿ç”¨å·²ç»åˆ›å»ºå¥½çš„å®ä¾‹
        synthesizer=synthesizer_agent,  # <-- ä½¿ç”¨å·²ç»åˆ›å»ºå¥½çš„å®ä¾‹
        gumbel_selector=gumbel_selector
    )
    
    print("\nâœ… All components initialized. Orchestrator is ready.")
    print("="*30)

    # --- 2a. é…ç½®æ‰¹é‡æµ‹è¯• ---
    DATASET_PATH = "data/ViDoSeek/rag_dataset.json"
    START_INDEX = 10  # ä»ç¬¬å‡ ä¸ªæ ·æœ¬å¼€å§‹æµ‹è¯•
    NUM_TO_TEST = 5  # å¸Œæœ›æµ‹è¯•çš„æ ·æœ¬æ•°é‡

    # --- 2b. åŠ è½½æµ‹è¯•æ ·æœ¬ ---
    print(f"\n[Batch Test] Loading dataset from {DATASET_PATH}...")
    try:
        with open(DATASET_PATH, "r", encoding="utf-8") as f:
            examples = json.load(f)["examples"]
        print(f"âœ… Dataset loaded. Found {len(examples)} total examples.")
    except FileNotFoundError:
        print(f"âŒ ERROR: Dataset file not found at {DATASET_PATH}. Cannot proceed.")
        return

    # --- 2c. å¾ªç¯æ‰§è¡Œæµ‹è¯• ---
    end_index = min(START_INDEX + NUM_TO_TEST, len(examples))
    for i in range(START_INDEX, end_index):
        sample = examples[i]
        query = sample.get("query")
        reference_answer = sample.get("reference_answer", "(This sample has no reference answer)")

        if not query:
            print(f"\n--- Skipping test case {i+1} (Index: {i}) because it has no query. ---")
            continue

        print("\n" + "#"*60)
        print(f"# Running Test Case {i+1} / {NUM_TO_TEST} (Dataset Index: {i})")
        print("#"*60)
        
        print(f"\n[User Query]: {query}")
        print("-" * 50)

        # --- æ‰§è¡Œ ---
        # 1. è·å–æŸ¥è¯¢åµŒå…¥
        # (å‡è®¾ query_embedder å·²åœ¨åˆå§‹åŒ–æ­¥éª¤ä¸­åˆ›å»º)
        from src.utils.embedding_utils import get_query_embedding
        query_embedding = get_query_embedding(query_embedder, query)
        
        # 2. è°ƒç”¨ Orchestrator ä¸€é”®è¿è¡Œ
        final_answer = orchestrator.run(
            query=query,
            query_embedding=query_embedding,
            initial_top_k=10
        )

        # --- æ­¥éª¤ 3: è¾“å‡ºç»“æœ (åŒ…å«å¯¹æ¯”) ---
        print("\n" + "="*30)
        print(f"ğŸ‰ Pipeline Execution Finished for Case {i+1} ğŸ‰")
        print(f"\n[Initial Query]: {query}")
        print("-" * 40)
        print(f"[Model's Final Answer]:\n{final_answer}")
        print("-" * 40)
        print(f"[Reference Answer]:\n{reference_answer}")
        print("="*30)


if __name__ == '__main__':
    main()