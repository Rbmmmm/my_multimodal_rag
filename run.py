# File: run.py

import os
import torch

from src.searcher.text_searcher import TextSearcher
from src.searcher.image_searcher import ImageSearcher
from src.searcher.table_searcher import TableSearcher
from src.searcher.SearchEngine import SearchEngine
from src.agent.seeker_agent import SeekerAgent
from src.agent.inspector_agent import InspectorAgent
from src.agent.synthesizer_agent import SynthesizerAgent
from src.models.gumbel_selector import GumbelModalSelector
from src.orchestrator import RAGOrchestrator
from src.llms.llm import LLM               
from src.utils.embedding_utils import QueryEmbedder      
from llama_index.core import Settings
import json

Settings.llm = None

def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºè®¾ç½®å’Œè¿è¡Œ RAG æµç¨‹ã€‚
    """
    # ==========================================================================
    # æ­¥éª¤ 1: åˆå§‹åŒ–æ‰€æœ‰ç³»ç»Ÿç»„ä»¶
    # ==========================================================================
    print("="*30)
    print("æ­¥éª¤ 1: åˆå§‹åŒ–æ‰€æœ‰ç³»ç»Ÿç»„ä»¶")
    
    # --- 1a. è®¾ç½® API Key å’Œè®¾å¤‡ ---
    print("\n[1a] è®¾ç½® vlm API Key...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if "sk" in os.environ["DASHSCOPE_API_KEY"]:
        print("[1a] âœ… vlm API Key å·²æˆåŠŸè®¾ç½®.")

    # --- 1b. åˆå§‹åŒ–æ ¸å¿ƒæ¨¡å‹ ---
    print("\n[1b] åˆå§‹åŒ– query åµŒå…¥æ¨¡å‹å’Œ vlm æ¨¡å‹...")
    query_embedder = QueryEmbedder(model_name="BAAI/bge-m3", device=device)
    vlm = LLM("qwen-vl-max")
    print("[1b] âœ… åˆå§‹åŒ– query åµŒå…¥æ¨¡å‹å’Œ vlm æ¨¡å‹ å·²æˆåŠŸåˆå§‹åŒ–.")

    # --- 1c. åˆ›å»ºæ£€ç´¢å™¨â€œå·¥å‚â€ (Retriever Factories) ---
    print("\n[1c] åˆ›å»ºæ£€ç´¢å™¨å·¥å‚...")
    retriever_factories = {
        "text": lambda: TextSearcher(
            dataset_name='ViDoSeek',
            mode='hybrid',
            node_dir_prefix='bge_ingestion' 
        ),
        "image": lambda: ImageSearcher(
            dataset_name='ViDoSeek',
            mode='vl_search',
            vl_node_dir_prefix='colqwen_ingestion' 
        ),
        "table": lambda: TableSearcher(
            dataset_name='ViDoSeek',
            mode='vl_search',
            vl_node_dir_prefix='colqwen_ingestion' 
        )
    }
    print("[1c] âœ… æ£€ç´¢å™¨å·¥å‚å®Œæˆ...")

    # --- 1d. åˆå§‹åŒ–æ”¯æŒæ‡’åŠ è½½çš„ SearchEngine ---
    print("\n[1d] åˆå§‹åŒ–æ”¯æŒæ‡’åŠ è½½çš„ SearchEngine...")
    search_engine = SearchEngine(retriever_factories=retriever_factories)
    print("[1d] âœ… æ‡’åŠ è½½ SearchEngine å·²æˆåŠŸåˆå§‹åŒ–.")

    # --- 1e. åˆå§‹åŒ– Gumbel æ¨¡æ€é€‰æ‹©å™¨ ---
    print("\n[1e] åˆå§‹åŒ– Gumbel æ¨¡æ€é€‰æ‹©å™¨...")
    gumbel_selector = GumbelModalSelector(
        input_dim=query_embedder.out_dim,
        num_choices=3 # 0=text, 1=image, 2=table
    ).to(device).eval()

    # åŠ è½½è®­ç»ƒå¥½çš„é€‰æ‹©å™¨æƒé‡
    ckpt_path = "checkpoints/modal_selector_best.pt"
    if os.path.exists(ckpt_path):
        gumbel_selector.load_state_dict(torch.load(ckpt_path, map_location=device))
        print(f"[1e] âœ… æˆåŠŸä» {ckpt_path} åŠ è½½ Gumbel Selector æƒé‡.")
    else:
        print("[1e] â„¹ï¸ No Gumbel Selector checkpoint found, using random initialization.")

    # --- 1f. åˆå§‹åŒ– seeker, inspector, synthesizer agents ---
    print("\n[1f] åˆå§‹åŒ– agents...")
    image_base_dir = "data/ViDoSeek/img" 
    seeker_agent = SeekerAgent(vlm=vlm, image_base_dir=image_base_dir)
    inspector_agent = InspectorAgent(vlm=vlm, image_base_dir=image_base_dir, reranker_model_name="BAAI/bge-reranker-large")
    synthesizer_agent = SynthesizerAgent(vlm=vlm, image_base_dir=image_base_dir)
    print("[1f] âœ… æ‰€æœ‰ agents å·²è¢«æˆåŠŸåˆå§‹åŒ–.")

    # --- 1g. ç»„è£…æ€»æŒ‡æŒ¥ (Orchestrator) ---
    print("\n[1g] åˆå§‹åŒ– orchestrator...")
    orchestrator = RAGOrchestrator(
        search_engine=search_engine,
        seeker=seeker_agent,          
        inspector=inspector_agent,    
        synthesizer=synthesizer_agent,  
        gumbel_selector=gumbel_selector
    )
    print("[1g] âœ… Orchestrator å·²æˆåŠŸåˆå§‹åŒ–.")
    
    print("\nâœ… æ­¥éª¤ä¸€ç»“æŸï¼šæ‰€æœ‰ç»„ä»¶å·²å°±ä½.")
    print("="*30)

    # ==========================================================================
    # æ­¥éª¤ 2: æ‰¹é‡æµ‹è¯•
    # ==========================================================================
    
    print("\næ­¥éª¤ 2: å¼€å§‹æ‰¹é‡æµ‹è¯•")
    
    # --- 2a. è®¾ç½®å‚æ•° ---
    print("\n[2a] è®¾ç½®æµ‹è¯•å‚æ•°...")
    DATASET_PATH = "data/ViDoSeek/rag_dataset.json"
    START_INDEX = 25  # ä»ç¬¬å‡ ä¸ªæ ·æœ¬å¼€å§‹æµ‹è¯•
    NUM_TO_TEST = 10  # å¸Œæœ›æµ‹è¯•çš„æ ·æœ¬æ•°é‡
    print("[2a] âœ… è®¾ç½®å®Œæˆ.")

    # --- 2b. åŠ è½½æµ‹è¯•æ ·æœ¬ ---
    print(f"\n[2b] ä» {DATASET_PATH} åŠ è½½æµ‹è¯•æ ·æœ¬...")
    try:
        with open(DATASET_PATH, "r", encoding="utf-8") as f:
            examples = json.load(f)["examples"]
        print(f"[2b] âœ… Dataset loaded. Found {len(examples)} total examples.")
    except FileNotFoundError:
        print(f"âŒ ERROR: Dataset file not found at {DATASET_PATH}. Cannot proceed.")
        return

    # --- 2c. å¾ªç¯æ‰§è¡Œæµ‹è¯• ---
    end_index = min(START_INDEX + NUM_TO_TEST, len(examples))
    cnt = 0
    for i in range(START_INDEX, end_index):
        cnt += 1
        sample = examples[i]
        query = sample.get("query")
        reference_answer = sample.get("reference_answer", "(This sample has no reference answer)")
        
        modality_index = None
        
        ### Test
        # modality = sample.get("meta_info")['source_type']
        
        # if modality == "text":
        #     modality_index = 0
        # elif modality == "2d_layout" or modality == "chart":
        #     modality_index = 1
        # elif modality == "table":
        #     modality_index = 2
        ### Test

        if not query:
            print(f"\n--- Skipping test case {i+1} (Index: {i}) because it has no query. ---")
            continue

        print("\n" + "#"*60)
        print(f"# Running Test Case {cnt} / {NUM_TO_TEST} (Dataset Index: {i})")
        print("#"*60)
        
        print(f"\n[User Query]: {query}")
        print("-" * 50)

        # --- æ‰§è¡Œ ---
        # 1. è·å–æŸ¥è¯¢åµŒå…¥
        # (å‡è®¾ query_embedder å·²åœ¨åˆå§‹åŒ–æ­¥éª¤ä¸­åˆ›å»º)
        from src.utils.embedding_utils import get_query_embedding
        query_embedding = get_query_embedding(query_embedder, query)
        
        # 2. è°ƒç”¨ Orchestrator ä¸€é”®è¿è¡Œ
        # å¯ä»¥æ‰‹åŠ¨è®¾ç½® top_k
        final_answer = orchestrator.run(
            query=query,
            query_embedding=query_embedding,
            initial_top_k=5,
            setted_modality_index = modality_index
        )

        # --- æ­¥éª¤ 3: è¾“å‡ºç»“æœ (åŒ…å«å¯¹æ¯”) ---
        print("\n" + "="*30)
        print(f"ğŸ‰ Pipeline Execution Finished for Case {i+1} ğŸ‰")
        print(f"\n[Initial Query]: {query}")
        print("-" * 40)
        print(f"[Model's Final Answer]:\n{final_answer}")
        print("-" * 40)
        print(f"[Reference Answer]:\n{reference_answer}")
        print("="*30)

if __name__ == '__main__':
    main()